{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "characteristic-being",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0e9af94-8270-47bd-aecd-08864f2289b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from configparser import ConfigParser\n",
    "\n",
    "config = ConfigParser()\n",
    "# create your own config.ini in root of project folder to store project configurations\n",
    "config.read('config.ini')\n",
    "\n",
    "pathfile = config.get('main', 'dirty_csv')  \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .appName(\"SparkFlight\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281cff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba261f3-ee93-4b3d-af7f-68331b3e04ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = spark.read.csv(pathfile, inferSchema='true', header='true', mode='PERMISSIVE', encoding='ISO-8859-1').limit(10**6).cache()\n",
    "data = spark.read.csv(pathfile, inferSchema='true', header='true', mode='PERMISSIVE', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd3b98a-ac58-4f35-b0e7-41b2ff657c98",
   "metadata": {},
   "source": [
    "### Analysis cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a8c31b3-1ac5-4c33-94c1-1d19e34208a5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+--------+--------------+--------+----+--------+---------------------------+------------------+------+-------------+-------+------+-------+-------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|ActualElapsedTime|AirTime|ArrDelay|CRSElapsedTime|DepDelay|Dest|Distance|LateAircraftAndCarrierDelay|NASAndWeatherDelay|Origin|SecurityDelay|TailNum|TaxiIn|TaxiOut|UniqueCarrier|    CRSDepTimeStamp|       DepTimestamp|       ArrTimestamp|    CRSArrTimestamp|\n",
      "+-----------------+-------+--------+--------------+--------+----+--------+---------------------------+------------------+------+-------------+-------+------+-------+-------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|               53|     32|      -8|            65|       4| PIT|     205|                          0|                 0|   DCA|            0| N443US|     7|     14|           US|2002-10-10 15:45:00|2002-10-10 15:49:00|2002-10-10 16:42:00|2002-10-10 16:50:00|\n",
      "|              164|    155|     -11|           175|       0| MCI|    1072|                          0|                 0|   MCO|            0|   N755|     2|      7|           WN|1999-12-02 16:10:00|1999-12-02 16:10:00|1999-12-02 18:54:00|1999-12-02 19:05:00|\n",
      "|               45|     29|       2|            48|       5| CMH|     116|                          0|                 0|   CVG|            0| N785CA|     3|     13|           OH|2006-06-19 10:30:00|2006-06-19 10:35:00|2006-06-19 11:20:00|2006-06-19 11:18:00|\n",
      "|               49|     37|       2|            47|       0| CLT|     156|                          0|                 0|   MYR|            0| N934VJ|     6|      6|           US|1997-01-02 10:48:00|1997-01-02 10:48:00|1997-01-02 11:37:00|1997-01-02 11:35:00|\n",
      "|               61|     40|      -3|            60|      -4| LAW|     140|                          0|                 0|   DFW|            0| N286AE|     7|     14|           MQ|2008-07-20 14:40:00|2008-07-20 14:36:00|2008-07-20 15:37:00|2008-07-20 15:40:00|\n",
      "|              150|    126|     -19|           169|       0| ATL|     903|                          0|                 0|   PVD|            0| N919DE|    14|     10|           DL|1998-10-15 17:45:00|1998-10-15 17:45:00|1998-10-15 20:15:00|1998-10-15 20:34:00|\n",
      "|              115|    103|      -5|           120|       0| SEA|     689|                          0|                 0|   SLC|            0|   N346|     2|     10|           WN|1998-06-16 06:40:00|1998-06-16 06:40:00|1998-06-16 08:35:00|1998-06-16 08:40:00|\n",
      "|              170|    135|      10|           161|       1| MCO|     950|                          0|                 0|   LGA|            0| N558AA|     3|     32|           AA|1998-10-22 08:00:00|1998-10-22 08:01:00|1998-10-22 10:51:00|1998-10-22 10:41:00|\n",
      "|              100|     81|      -2|            97|      -5| ATL|     300|                          0|                 0|   GNV|            0| N632AS|    10|      9|           EV|2006-11-06 10:45:00|2006-11-06 10:40:00|2006-11-06 12:20:00|2006-11-06 12:22:00|\n",
      "|               79|     70|     -16|           100|       5| PIT|     553|                          0|                 0|   STL|            0| N970VJ|     5|      4|           US|1995-11-19 12:20:00|1995-11-19 12:25:00|1995-11-19 13:44:00|1995-11-19 14:00:00|\n",
      "|              133|    117|      40|           125|      32| BOS|     867|                          0|                 0|   ORD|            0| N7280U|     5|     11|           UA|1997-01-30 10:44:00|1997-01-30 11:16:00|1997-01-30 13:29:00|1997-01-30 12:49:00|\n",
      "|               33|     22|      -3|            34|      -2| HNL|     100|                          0|                 0|   OGG|            0| N477HA|     5|      6|           HA|2007-11-06 18:45:00|2007-11-06 18:43:00|2007-11-06 19:16:00|2007-11-06 19:19:00|\n",
      "|               57|     31|      -9|            56|     -10| ATL|     152|                          0|                 0|   TYS|            0| N846AS|     9|     17|           EV|2005-02-04 16:12:00|2005-02-04 16:02:00|2005-02-04 16:59:00|2005-02-04 17:08:00|\n",
      "|               79|     65|      13|            85|      19| SMF|     480|                          0|                 0|   SAN|            0|   N358|     3|     11|           WN|1999-08-04 14:20:00|1999-08-04 14:39:00|1999-08-04 15:58:00|1999-08-04 15:45:00|\n",
      "|               95|     74|     136|           112|     153| SGF|     563|                        136|                 0|   ATL|            0| N854AS|     3|     18|           EV|2006-07-30 20:35:00|2006-07-30 23:08:00|2006-07-31 00:43:00|2006-07-30 22:27:00|\n",
      "|              105|     61|       3|            99|      -3| PIT|     335|                          0|                 0|   LGA|            0| N934VJ|     7|     37|           US|2000-01-07 18:35:00|2000-01-07 18:32:00|2000-01-07 20:17:00|2000-01-07 20:14:00|\n",
      "|              143|    122|       4|           140|       1| ATL|     780|                          0|                 0|   HPN|            0| N996AT|    11|     10|           FL|2006-12-21 06:00:00|2006-12-21 06:01:00|2006-12-21 08:24:00|2006-12-21 08:20:00|\n",
      "|               67|     52|      -4|            68|      -3| PVD|     328|                          0|                 0|   BWI|            0| N278AU|     6|      9|           US|1998-08-23 14:10:00|1998-08-23 14:07:00|1998-08-23 15:14:00|1998-08-23 15:18:00|\n",
      "|               97|     76|       2|           102|       7| CLE|     404|                          0|                 0|   EWR|            0| N27610|     4|     17|           CO|2000-12-23 12:15:00|2000-12-23 12:22:00|2000-12-23 13:59:00|2000-12-23 13:57:00|\n",
      "|              245|    228|     -14|           261|       2| DEN|    1619|                          0|                 0|   LGA|            0| N327UA|     5|     12|           UA|1995-12-26 18:00:00|1995-12-26 18:02:00|1995-12-26 22:07:00|1995-12-26 22:21:00|\n",
      "+-----------------+-------+--------+--------------+--------+----+--------+---------------------------+------------------+------+-------------+-------+------+-------+-------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e561e080-2d3f-4383-b397-c2d57f3fb967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if read.csv(inferSchema='true') inferred the column types correctly\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083297a8-2482-49b8-bfb5-fa643dcdd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d49369-e517-4223-be95-203bc5e10f47",
   "metadata": {},
   "source": [
    "### {preprocess step} Filter out NA and Null values in all columns saved in var colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d804ab-126b-4668-98fe-e023c5848436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows that have NA values in following columns that are assumed to be important for potential FDs:\n",
    "colnames = [\"ActualElapsedTime\", \"AirTime\", \"ArrDelay\", \"ArrTime\", \"CRSArrTime\", \"CRSDepTime\", \"CRSElapsedTime\", \"DayOfWeek\", \"DayofMonth\", \"DepDelay\", \"DepTime\", \"Dest\", \"Distance\", \"FlightNum\", \"Month\", \"Origin\", \"TailNum\", \"TaxiIn\", \"TaxiOut\", \"UniqueCarrier\", \"Year\"]\n",
    "# Specific delay columns like CarrierDelay and WeatherDelay with NA or Nul values need to have these values changed to 0 instead of filtered out.\n",
    "# Tailnum still has some duplicated that do not have the same right UniqueCarrier\n",
    "# also has some UNKNOW columns for TailNum. We keep these for soft dependency\n",
    "\n",
    "# pyspark does not recognize csv \"NA\" strings as NaN values.\n",
    "# This is reflected in the output of data.dtypes: columns that normally only have numeric values were not inferred as int type.\n",
    "# Also remove these rows by filtering them out.\n",
    "data_filt_nan = data.filter(data.ActualElapsedTime.isNotNull() & \n",
    "                            (data.ActualElapsedTime != \"NA\") &\n",
    "                            data.AirTime.isNotNull() &\n",
    "                            (data.AirTime != \"NA\") &\n",
    "                            data.ArrDelay.isNotNull() &\n",
    "                            (data.ArrDelay != \"NA\") &\n",
    "                            data.ArrTime.isNotNull() &\n",
    "                            (data.ArrTime != \"NA\") &              \n",
    "                            data.CRSArrTime.isNotNull() & \n",
    "                            data.CRSDepTime.isNotNull() &\n",
    "                            data.CRSElapsedTime.isNotNull() &\n",
    "                            (data.CRSElapsedTime != \"NA\") &\n",
    "                            data.DayOfWeek.isNotNull() &\n",
    "                            data.DayofMonth.isNotNull() &\n",
    "                            data.DepDelay.isNotNull() &\n",
    "                            (data.DepDelay != \"NA\") &\n",
    "                            data.DepTime.isNotNull() &\n",
    "                            (data.DepTime != \"NA\") &\n",
    "                            data.Dest.isNotNull() &\n",
    "                            (data.Dest != \"NA\") &\n",
    "                            data.Distance.isNotNull() &\n",
    "                            (data.Distance != \"NA\") &\n",
    "                            data.FlightNum.isNotNull() &\n",
    "                            data.Month.isNotNull() &\n",
    "                            data.Origin.isNotNull() &\n",
    "                            (data.Origin != \"NA\") &\n",
    "                            data.TailNum.isNotNull() &\n",
    "                            (data.TailNum != \"NA\") &\n",
    "                            data.TaxiIn.isNotNull() &\n",
    "                            (data.TaxiIn != \"NA\") &\n",
    "                            data.TaxiOut.isNotNull() &\n",
    "                            (data.TaxiOut != \"NA\") &\n",
    "                            data.UniqueCarrier.isNotNull() &\n",
    "                            (data.UniqueCarrier != \"NA\") &\n",
    "                            data.Year.isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd26d3c-830f-4a7e-9910-e6b1d14d276c",
   "metadata": {},
   "source": [
    "### Analysis cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda47d0a-50e9-4873-be95-8473486759c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this suggests the correct amount of rows are removed\n",
    "data_filt_nan.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd29a09-a5f8-44b8-8858-39ca55d2e88d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in the original dataframe there are still columns with NaN value that should be filtered out\n",
    "data.select(*colnames).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab78e6-8176-49f6-b231-463182e87266",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NaN values in colnames should be filtered out\n",
    "# TailNum still shows weird ascci signs sometimes.\n",
    "data_filt_nan.select(*colnames).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ee787-7163-4619-892c-847fbbc33f90",
   "metadata": {},
   "source": [
    "### {preprocess step} Take +- 15 mil rows now before any further transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072bc5e7-c418-452b-ab04-f70b1c904460",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_limit = data_filt_nan.limit(15_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9bbc3-d687-46c3-bfad-068f6f611a5f",
   "metadata": {},
   "source": [
    "### Analysis cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bdf757-1a8e-4dfa-8cee-d8de061133b6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data.printSchema()\n",
    "# data.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb58c9-97e4-438c-a9eb-efd84692d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_filt_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7943b5ba-125e-45b7-864a-dd8fa54658b8",
   "metadata": {},
   "source": [
    "### {preprocess step} Transform certain existing columns that store minutes as string to timestamp type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e5f2d9-4b94-4682-bc82-d8b46739e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform existing date columns to be able to use them for to_timestamp transformation\n",
    "# this can then be used to calculate with timestamps for total time difference between two attributes (delta dependencies)\n",
    "# example: transform 1 minute to hhm:mm -> 00:01\n",
    "date_cols = [\"Year\", \"Month\", \"DayofMonth\", \"CRSDepTime\"]\n",
    "data_timestamp = (data_limit.withColumn('Month', F.when(F.length(F.col('Month')) == 1, F.concat(F.lit('0'), F.col('Month'))).otherwise(F.col('Month')))\n",
    "                    .withColumn('DayofMonth', F.when(F.length(F.col('DayofMonth')) == 1, F.concat(F.lit('0'), F.col('DayofMonth'))).otherwise(F.col('DayofMonth')))\n",
    "                    .withColumn('CRSDepTime', F.when(F.length(F.col('CRSDepTime')) == 1, F.concat(F.lit('000'), F.col('CRSDepTime')))\n",
    "                                        .when(F.length(F.col('CRSDepTime')) == 2, F.concat(F.lit('00'), F.col('CRSDepTime')))\n",
    "                                        .when(F.length(F.col('CRSDepTime')) == 3, F.concat(F.lit('0'), F.col('CRSDepTime')))\n",
    "                                        .otherwise(F.col('CRSDepTime')))\n",
    "                    .withColumn('CRSDepTimeStamp', F.to_timestamp(F.concat(*date_cols), format='yyyyMMddHHmm'))\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcd40ef-2f8d-4af2-a03e-83ec9d4f07dd",
   "metadata": {},
   "source": [
    "### Analysis cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f475605-99cd-47f9-81db-ac8c2c3e3770",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_timestamp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39172ae2-d5da-4726-8779-f1ed80501945",
   "metadata": {},
   "source": [
    "### {preprocess step} Transform DepTime with minutes only to full Timestamp with a date as new column using DepDelay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d09031-e29e-4fc1-bc00-437e9fbadb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deptimestamp calculating by using CRSDepTimeStamp and DepDelay because actual DepTime and CRSDepTime can differ by at most one full day = 1440 mins\n",
    "data_deptime = data_timestamp.withColumn(\"DepTimestamp\", (col(\"CRSDepTimeStamp\").cast(\"long\") + (col(\"DepDelay\").cast(\"long\"))*60).cast(\"timestamp\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65628ce-f24b-42a2-8ae0-9d47eea60b68",
   "metadata": {},
   "source": [
    "### Analysis cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340cadd-5c1d-4e42-af57-c6fa901580fe",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_deptime.show(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3cf832-73ef-4798-9282-a70cc26884aa",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_deptime.filter(data_deptime.DepDelay > 1300).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf6651-b5a1-405e-af7f-51177ef9a219",
   "metadata": {},
   "source": [
    "### {preprocess step} Transform ArrTime with minutes only to full Timestamp with a date as new column by adding ActualElapsedTime to DepTimestamp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fd2ba-36ce-4805-a4cb-f0e7e88e4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arrtime = data_deptime.withColumn(\"ArrTimestamp\", (col(\"DepTimestamp\").cast(\"long\") + (col(\"ActualElapsedTime\").cast(\"long\"))*60).cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f30d06-7d4e-4682-84d2-09b366dbecab",
   "metadata": {},
   "source": [
    "### Analysis cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc4166-f5ca-4497-b710-772aeb8d0ee9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_arrtime.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75809603-1bd3-42d9-835a-52bd801dd2dc",
   "metadata": {},
   "source": [
    "### {preprocess step} Transform CRSArrTime with minutes only to full Timestamp with a date as new column by adding CRSElapsedTime to CRSDepTime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf9ad2a-fee1-4186-85ac-3f226ee26d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRSArrTimestamp calculating by using CRSDepTimestamp and CRSElapsedTime\n",
    "data_crs_arrtime = data_arrtime.withColumn(\"CRSArrTimestamp\", (col(\"CRSDepTimestamp\").cast(\"long\") + (col(\"CRSElapsedTime\").cast(\"long\"))*60).cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fc148a-62bf-4a97-96bc-4cf7143a4225",
   "metadata": {},
   "source": [
    "### Analysis cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2197ccdf-8f39-4b8f-88b5-f0307662dcb7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_crs_arrtime.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3794c-905e-4f0c-b143-0e36b001ecfb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  check if there are duplicates in DepTimestamp for delta dependency\n",
    "data_crs_arrtime.groupBy(\"DepTimestamp\").count().orderBy(col(\"count\").desc()).show(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a06fa4-f3e1-406a-b1f8-c0ca2c792c9a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  check if there are duplicates in ArrTimestamp for delta dependency\n",
    "data_crs_arrtime.groupBy(\"ArrTimestamp\").count().orderBy(col(\"count\").desc()).show(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8337c81-f20e-4a6a-bdc5-87bdc98afed7",
   "metadata": {},
   "source": [
    "### {preprocess step} Filter out rows that have have cancelled == 1 or Diverted == 1 as they leave out very valuable info\n",
    "### This is done before removing these columns to filter out low information rows first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c802186-dba9-4267-8876-1358ceb2861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows with cancelled == 1 or Diverted == 1 were probably already filtered because of filtered out NaN values for the important time columns\n",
    "# Just to be sure\n",
    "data_filt_div_canc = data_crs_arrtime.filter((data_crs_arrtime.Cancelled == 0) | (data_crs_arrtime.Diverted == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c327e-e34d-48c7-9a45-9e06cd2160ae",
   "metadata": {},
   "source": [
    "### Analysis cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f906d-3808-4222-ad8c-90ecf624b787",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if there are no rows left with cancelled == 1\n",
    "data_filt_div_canc.orderBy(col(\"Cancelled\").desc()).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab51b2-4fd8-42a1-aaf3-ba4e152a5f91",
   "metadata": {},
   "source": [
    "### {preprocess step} Drop columns that have already been transformed to other columns or columns that seem less useful\n",
    "- Original dataset has 28 columns which we want to try to reduce due to computational complexity\n",
    "- The date columns like Year and Month have already been transformed to CRSDepTimeStamp\n",
    "- Flightnumber only has a few thousand unique numbers over a million records and seems to have no correlation with others\n",
    "- CancellationCode, Cancelled and Diverted seem to be very messy and occur only very rarely\n",
    "    - Cancelled and Diverted only have two possible values: 1 and 0 and 1 only occurs very rarely\n",
    "    - CancellationCode is also not always set when a flight is cancelled\n",
    "- After this step number of cols is reduced from 28 -> 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6625f44d-9b9e-4b56-9787-e285c26c4df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"CancellationCode\", \"Cancelled\", \"Diverted\", \"Year\", \"Month\", \"DayOfWeek\", \"DayofMonth\", \"ArrTime\", \"CRSArrTime\", \"CRSDepTime\", \"DepTime\", \"FlightNum\"]\n",
    "# drop as many (mostly) useless columns as we can\n",
    "data_drop_cols = data_filt_div_canc.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9686fc7e-e4f4-4f98-9090-d28b3460982d",
   "metadata": {},
   "source": [
    "### Analysis cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b71e5e-fb71-44a8-b0d5-cb362c2dda62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_drop_cols.show(20)\n",
    "len(data_drop_cols.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed8b28-58d9-41fa-9fcb-2c2d39fa3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drop_cols.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae781d0-22cd-4848-a526-d21d5d50f07d",
   "metadata": {},
   "source": [
    "### {preprocess step} Combine several specific delay columns to one column to reduce amount of columns\n",
    "### Also cast columns that store minutes to int (turned out to be bigint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb3f40-3331-42cc-bef4-2622b85fa23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine LateAircraftDelay and CarrierDelay into one column.\n",
    "# Combine NASDelay and WeatherDelay into one column\n",
    "# Cast columns with string type to bigint (will get transformed later)\n",
    "def transform_and_cast_cols(x):\n",
    "    ActualElapsedTime= int(x.ActualElapsedTime)\n",
    "    AirTime= int(x.AirTime)\n",
    "    ArrDelay= int(x.ArrDelay)\n",
    "    CRSElapsedTime= int(x.CRSElapsedTime)\n",
    "    DepDelay= int(x.DepDelay)\n",
    "    Dest= x.Dest\n",
    "    Distance= int(x.Distance)\n",
    "    LateAircraftDelay = x.LateAircraftDelay\n",
    "    CarrierDelay = x.CarrierDelay\n",
    "    \n",
    "    if LateAircraftDelay == \"NA\":\n",
    "        LateAircraftDelay = 0\n",
    "    if CarrierDelay == \"NA\":\n",
    "        CarrierDelay = 0 \n",
    "    \n",
    "    LateAircraftAndCarrierDelay = int(LateAircraftDelay) + int(CarrierDelay)\n",
    "    \n",
    "    NASDelay = x.NASDelay\n",
    "    WeatherDelay = x.WeatherDelay\n",
    "    if NASDelay == \"NA\":\n",
    "        NASDelay = 0\n",
    "    if WeatherDelay == \"NA\":\n",
    "        WeatherDelay = 0 \n",
    "    \n",
    "    NASAndWeatherDelay= int(NASDelay) + int(WeatherDelay)\n",
    "    Origin=x.Origin\n",
    "    SecurityDelay = x.SecurityDelay\n",
    "    if SecurityDelay == \"NA\":\n",
    "        SecurityDelay = 0\n",
    "        \n",
    "    SecurityDelay= int(SecurityDelay)\n",
    "    TailNum=x.TailNum\n",
    "    TaxiIn=int(x.TaxiIn)\n",
    "    TaxiOut=int(x.TaxiOut)\n",
    "    UniqueCarrier=x.UniqueCarrier\n",
    "    CRSDepTimeStamp=x.CRSDepTimeStamp\n",
    "    DepTimestamp=x.DepTimestamp\n",
    "    ArrTimestamp=x.ArrTimestamp\n",
    "    CRSArrTimestamp=x.CRSArrTimestamp\n",
    "    return (ActualElapsedTime,\n",
    "            AirTime,\n",
    "            ArrDelay,\n",
    "            CRSElapsedTime,\n",
    "            DepDelay,\n",
    "            Dest,\n",
    "            Distance,\n",
    "            LateAircraftAndCarrierDelay,\n",
    "            NASAndWeatherDelay,\n",
    "            Origin,\n",
    "            SecurityDelay,\n",
    "            TailNum,\n",
    "            TaxiIn,\n",
    "            TaxiOut,\n",
    "            UniqueCarrier,\n",
    "            CRSDepTimeStamp,\n",
    "            DepTimestamp,\n",
    "            ArrTimestamp,\n",
    "            CRSArrTimestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a300d0-23a0-4ddd-a553-7894534b846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first transform dataframe to rdd as only rdd has map transform function\n",
    "rdd_trans_n_cast_cols = data_drop_cols.rdd.map(lambda x: transform_and_cast_cols(x))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963f44c-567c-49df-b021-83c93e717b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform rdd back to dataframe\n",
    "# have to specify columns again because rdd has to tabular structure\n",
    "data_trans_n_cast_cols = rdd_trans_n_cast_cols.toDF(['ActualElapsedTime',\n",
    "                                 'AirTime',\n",
    "                                 'ArrDelay',\n",
    "                                 'CRSElapsedTime',\n",
    "                                 'DepDelay',\n",
    "                                 'Dest',\n",
    "                                 'Distance',\n",
    "                                 'LateAircraftAndCarrierDelay',\n",
    "                                 'NASAndWeatherDelay',\n",
    "                                 'Origin',\n",
    "                                 'SecurityDelay',\n",
    "                                 'TailNum',\n",
    "                                 'TaxiIn',\n",
    "                                 'TaxiOut',\n",
    "                                 'UniqueCarrier',\n",
    "                                 'CRSDepTimeStamp',\n",
    "                                 'DepTimestamp',\n",
    "                                 'ArrTimestamp',\n",
    "                                 'CRSArrTimestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c49c4-3c27-4067-8419-6ab9df966147",
   "metadata": {},
   "source": [
    "### Analysis cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68448ec6-8cf4-4252-8418-1fa958b67b0f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_trans_n_cast_cols.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc97f8-43f6-4971-906e-5f7b2e58cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans_n_cast_cols.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c637c39-c531-4002-a26e-3d78ce298a79",
   "metadata": {},
   "source": [
    "### {preprocess step} Cast bigint columns to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a1f92-c141-4fb1-8210-065202a7c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_col_list = ['ActualElapsedTime', 'AirTime', 'ArrDelay', 'CRSElapsedTime', 'DepDelay', 'Distance', 'LateAircraftAndCarrierDelay', 'NASAndWeatherDelay', 'SecurityDelay', 'TaxiIn', 'TaxiOut']\n",
    "for col in int_col_list:\n",
    "    data_trans_n_cast_cols = data_trans_n_cast_cols.withColumn(col, data_trans_n_cast_cols[col].cast('integer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ea15b-a73d-405f-bf62-16a58c102a13",
   "metadata": {},
   "source": [
    "### Analysis cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b487ff7-9e9b-40c9-b9d1-b66e13c9b48b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_trans_n_cast_cols.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c2d53-7cd6-47d7-b262-1f101f677cb6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_trans_n_cast_cols.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8560b7e-2bd4-44d7-a90e-1601bd9e545c",
   "metadata": {},
   "source": [
    "### Save dataframe to CSV again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2258ad8-6d80-4d11-8837-559e7f584580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will partition the dataframe into several csv files stored under a .csv directory\n",
    "# to load in the preprocessed csv again you may use the path to the .csv directory\n",
    "# spark will automatically load in all csv parts included in the directory\n",
    "data_trans_n_cast_cols.write.csv('preprocessed_data.csv', header = True)\n",
    "\n",
    "# started at 19:26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a780270c-ccf1-48f9-97b9-72126b37ba07",
   "metadata": {},
   "source": [
    "### Test the saved preprocessed CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf3959-0ca4-43aa-8d8e-588daadff19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_data = spark.read.csv('preprocessed_data.csv', inferSchema='true', header='true', mode='PERMISSIVE', encoding='ISO-8859-1').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece7c82-84a0-40b9-81bb-5b5faccba3f3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preproc_data.show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e01e688-ffd3-4793-9346-510fbf9fac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfa197-47a7-4c4d-8047-a811797bc902",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f540d6d-a701-4ac4-96f8-d28a54d08e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preproc_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-client",
   "metadata": {},
   "source": [
    "## Correlation filter\n",
    "This is my proposed solution for batching. The idea is simple:\n",
    "1) First query the 16.000 elements\n",
    "2) Then check if two columns are correlated. And exstend from there \n",
    "3) ...\n",
    "4) Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dedecc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def unique_permutations(iterable, r=None):\n",
    "    previous = tuple()\n",
    "    for p in permutations(sorted(iterable), r):\n",
    "        if p > previous:\n",
    "            previous = p\n",
    "            yield p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec023b8",
   "metadata": {},
   "source": [
    "Step 1: Transform all the categorical columns to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c20f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_limit = data.limit(16000) #this could be a pre-processing step\n",
    "indexer =  data_limit.select(\"*\").toPandas()\n",
    "for col in indexer.select_dtypes(exclude=['number']).columns:\n",
    "     indexer[col] = pd.Categorical(indexer[col], categories=indexer[col].unique()).codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3e8d259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_based_on_pvalue(sampledataset):\n",
    "    def simple_hash(x): \n",
    "        return (x * 5399 + 7691 ) % 71\n",
    "    \n",
    "    def _filter(dependency):\n",
    "        lhs, rhs = dependency\n",
    "        \n",
    "        # first we construct a column for the left hand side\n",
    "        sampledataset[\"lhs\"] = 0\n",
    "        for item in lhs:\n",
    "            sampledataset[\"lhs\"] = sampledataset['lhs'] * 100_000 + sampledataset[item]\n",
    "        \n",
    "        sampledataset[\"lhs\"] = simple_hash(sampledataset[\"lhs\"])\n",
    "        sampledataset[\"rhs\"] = simple_hash(sampledataset[rhs])\n",
    "        \n",
    "        crosstab =  pd.crosstab(sampledataset['lhs'], sampledataset['rhs'])\n",
    "        _, p, _, _ = stats.chi2_contingency(crosstab)\n",
    "        \n",
    "        return p < 0.05\n",
    "    return _filter\n",
    "\n",
    "def find_correlected_dependencies_with_spark(dependencies_to_check):\n",
    "    rdd=spark.sparkContext.parallelize(dependencies_to_check)\n",
    "    filtered = rdd.filter(filter_based_on_pvalue(indexer))\n",
    "    return filtered.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a3a9bc7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['ActualElapsedTime'], 'AirTime'), (['ActualElapsedTime'], 'ArrDelay'), (['ActualElapsedTime'], 'CRSElapsedTime'), (['ActualElapsedTime'], 'DepDelay'), (['ActualElapsedTime'], 'DepTimestamp'), (['ActualElapsedTime'], 'Dest'), (['ActualElapsedTime'], 'Distance'), (['ActualElapsedTime'], 'Origin'), (['ActualElapsedTime'], 'TaxiIn'), (['ActualElapsedTime'], 'TaxiOut'), (['ActualElapsedTime'], 'UniqueCarrier'), (['AirTime'], 'CRSElapsedTime'), (['AirTime'], 'Dest'), (['AirTime'], 'Distance'), (['AirTime'], 'NASAndWeatherDelay'), (['AirTime'], 'Origin'), (['AirTime'], 'TaxiIn'), (['AirTime'], 'TaxiOut'), (['AirTime'], 'UniqueCarrier'), (['ArrDelay'], 'CRSElapsedTime'), (['ArrDelay'], 'DepDelay'), (['ArrDelay'], 'Dest'), (['ArrDelay'], 'LateAircraftAndCarrierDelay'), (['ArrDelay'], 'NASAndWeatherDelay'), (['ArrDelay'], 'SecurityDelay'), (['ArrDelay'], 'TaxiIn'), (['ArrDelay'], 'TaxiOut'), (['ArrDelay'], 'UniqueCarrier'), (['CRSElapsedTime'], 'DepDelay'), (['CRSElapsedTime'], 'Dest'), (['CRSElapsedTime'], 'Distance'), (['CRSElapsedTime'], 'NASAndWeatherDelay'), (['CRSElapsedTime'], 'Origin'), (['CRSElapsedTime'], 'TaxiIn'), (['CRSElapsedTime'], 'TaxiOut'), (['CRSElapsedTime'], 'UniqueCarrier'), (['DepDelay'], 'Dest'), (['DepDelay'], 'Distance'), (['DepDelay'], 'LateAircraftAndCarrierDelay'), (['DepDelay'], 'NASAndWeatherDelay'), (['DepDelay'], 'Origin'), (['DepDelay'], 'SecurityDelay'), (['DepDelay'], 'TailNum'), (['DepDelay'], 'TaxiIn'), (['DepDelay'], 'TaxiOut'), (['DepDelay'], 'UniqueCarrier'), (['Dest'], 'Distance'), (['Dest'], 'LateAircraftAndCarrierDelay'), (['Dest'], 'Origin'), (['Dest'], 'TailNum'), (['Dest'], 'TaxiIn'), (['Dest'], 'TaxiOut'), (['Dest'], 'UniqueCarrier'), (['Distance'], 'Origin'), (['Distance'], 'SecurityDelay'), (['Distance'], 'TailNum'), (['Distance'], 'TaxiIn'), (['Distance'], 'TaxiOut'), (['Distance'], 'UniqueCarrier'), (['LateAircraftAndCarrierDelay'], 'NASAndWeatherDelay'), (['LateAircraftAndCarrierDelay'], 'SecurityDelay'), (['LateAircraftAndCarrierDelay'], 'TaxiIn'), (['LateAircraftAndCarrierDelay'], 'TaxiOut'), (['LateAircraftAndCarrierDelay'], 'UniqueCarrier'), (['NASAndWeatherDelay'], 'SecurityDelay'), (['NASAndWeatherDelay'], 'TaxiIn'), (['NASAndWeatherDelay'], 'TaxiOut'), (['NASAndWeatherDelay'], 'UniqueCarrier'), (['Origin'], 'TailNum'), (['Origin'], 'TaxiIn'), (['Origin'], 'TaxiOut'), (['Origin'], 'UniqueCarrier'), (['SecurityDelay'], 'UniqueCarrier'), (['TailNum'], 'TaxiOut'), (['TailNum'], 'UniqueCarrier'), (['TaxiIn'], 'TaxiOut'), (['TaxiIn'], 'UniqueCarrier'), (['TaxiOut'], 'UniqueCarrier')]\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from scipy import stats\n",
    "\n",
    "# in this list we add all the elements that must be checked by spark\n",
    "q = []\n",
    "\n",
    "# We have already seen these combinations\n",
    "cachedCombinations = []\n",
    "\n",
    "foundcorrelations = []\n",
    "\n",
    "# first we add all the single columns. For instance A->B, B->C BUT NOT B-> A\n",
    "for p in unique_permutations(data.columns, 2):\n",
    "    if (tuple([p[-1]]),p[0:-1]) not in cachedCombinations and len(p[0:-1]) > 0:\n",
    "        cachedCombinations.append((p[0:-1],tuple([p[-1]])))\n",
    "        q.append((list(p[0:-1]),p[-1]))\n",
    "\n",
    "# single_correlated is a list with tuples, the 0th index of the tuple is a list for the LHS the\n",
    "# 1th index is a single column foro the RHS \n",
    "single_correlated = find_correlected_dependencies_with_spark(q)  \n",
    "foundcorrelations = foundcorrelations + single_correlated\n",
    "\n",
    "\n",
    "# this is where I stop, we now should construct dependecies with 2 LHS columns, and 3 etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_limit.toDF(*data_limit.columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71923a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
