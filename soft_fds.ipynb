{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70eb68aa-71bb-4956-91d5-9602a0e9f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from configparser import ConfigParser\n",
    "\n",
    "config = ConfigParser()\n",
    "# create your own config.ini in root of project folder to store project configurations\n",
    "config.read('config.ini')\n",
    "\n",
    "pathfile = config.get('main', 'dirty_csv')  \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .appName(\"SparkFlight\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7270cd0-efa4-476b-89b2-a42953bf5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read preprocessed data (15mil rows)\n",
    "preproc_data = spark.read.csv('preprocessed_data.csv', inferSchema='true', header='true', mode='PERMISSIVE', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb48cb2-f356-4a9b-ad9c-b705f26286cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "from itertools import combinations, product\n",
    "\n",
    "def gen_col_combs(col_names, max_lhs):\n",
    "    col_combs = {}\n",
    "    for i in range(1, max_lhs+1):\n",
    "#       for each diff lhs size, store col combs in dict\n",
    "        col_combs[f'lhs_{i}'] = []\n",
    "        combs = []\n",
    "#       if left hand side = 1, A=>B equals B=>A for the contingency table\n",
    "#       so just combinations of 2 needed\n",
    "        if i == 1:\n",
    "            col_combs[f'lhs_{i}'] = list(combinations(col_names, i+1))\n",
    "        else:\n",
    "#           for left hand side > 1\n",
    "#           make combinations of lhs and cartesian product these with all column names on rhs - column names that occur in lhs (trivial)\n",
    "            combs = combinations(col_names, i)\n",
    "            for comb in combs:\n",
    "                for col in col_names:\n",
    "                    if col not in comb:\n",
    "                        col_combs[f'lhs_{i}'].append(comb + (col,))\n",
    "    return col_combs\n",
    "\n",
    "def gen_contin_cells(x, lhs_size):\n",
    "    combs = []\n",
    "    for comb in broadcast_col_names.value[f'lhs_{lhs_size}']:\n",
    "# rows in form: (((lhs, rhs), (value(s) lhs, value rhs)), count)\n",
    "        combs.append(((comb, tuple(x[i] for i in comb)),1))\n",
    "    return combs\n",
    "\n",
    "def calc_combinations(x):\n",
    "#     combinations of 2\n",
    "    return factorial(x)/(factorial(2)*factorial(x-2)) if x >= 2 else 0\n",
    "\n",
    "def map_value_to_combs_count(rdd_row):\n",
    "    lhs_plus_rhs = rdd_row[0][0]\n",
    "#     value of rhs is always last item. Strip it to only depend on value of lhs in key\n",
    "    values_lhs_tup = rdd_row[0][1][:-1]\n",
    "    reduced_count_rows = rdd_row[1]\n",
    "    return ((lhs_plus_rhs, values_lhs_tup), calc_combinations(reduced_count_rows))\n",
    "        \n",
    "def calc_percentage(x, y):\n",
    "#     normalize to percentage value in [0, 1]\n",
    "    result = y/x if x > y else x/y\n",
    "#     if normalized percentage value = 1 then set to 1.5 instead so this value cannot be confused\n",
    "#     with a possible total 2 comb count of 1 for lhs\n",
    "#     as total combination count always has to be integer\n",
    "    result = 1.5 if result == 1 else result \n",
    "    return result\n",
    "\n",
    "def map_total_comb_to_zero_perc(rdd_row):\n",
    "    lhs_plus_rhs = rdd_row[0][0] \n",
    "    value = rdd_row[1]\n",
    "#     rdd_row could be row with total 2 comb count per lhs as value that did not get normalized to [0, 1] range as percentage\n",
    "#     as all rows for that lhs had unique rhs values so there was no row with possible combinations of 2 equal lhs + rhs per lhs\n",
    "#     to reduce with. Rows with unique lhs + rhs were filtered in step #3. \n",
    "#     lhs that has no possible combinations of 2 equal lhs + rhs per lhs will be set to 0%\n",
    "#     otherwise rdd_row has normalized percentage as value but in the case of percentage being 100% value = 1.5 instead of 1.\n",
    "    if value >= 1 and value != 1.5:\n",
    "        value = 0\n",
    "    return (lhs_plus_rhs, value)\n",
    "    \n",
    "def rem_rhs_value_from_key(rdd_row):\n",
    "    lhs_plus_rhs = rdd_row[0][0]\n",
    "#     value of rhs is always last item. Strip it to only depend on value of lhs in key\n",
    "    values_lhs_tup = rdd_row[0][1][:-1]\n",
    "    value = rdd_row[1]\n",
    "    return ((lhs_plus_rhs, values_lhs_tup), value)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd315bf5-c73d-445e-84f4-b34e7d7a590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  get a sample. This is just for testing locally. Should be whole preprocessed dataset.\n",
    "# len_sample = 16_000\n",
    "# len_preproc_data = 15_000_000\n",
    "# cont_sample_data = preproc_data.sample(True, fraction=len_sample/len_preproc_data).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f8f4e-f6bc-4852-b0f3-7e307aab5ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_lhs = 3\n",
    "col_combs = gen_col_combs(preproc_data.columns, max_lhs)\n",
    "# broadcast column combinations for efficient copying of immutable column combs list to nodes\n",
    "broadcast_col_names = spark.sparkContext.broadcast(col_combs)\n",
    "perc_threshold = 0.7\n",
    "\n",
    "for i in range(1, max_lhs+1):\n",
    "# #    sample only for local use to test\n",
    "#     flat_columns = cont_sample_data.rdd.flatMap(lambda x: gen_contin_cells(x, lhs_size=i)) # 1\n",
    "#     create all column combs per row\n",
    "    flat_columns = preproc_data.rdd.flatMap(lambda x: gen_contin_cells(x, lhs_size=i)) # 1\n",
    "#     cache or not?? only used one time extra this rdd later? check if this actually wins time\n",
    "    c_flat_columns = flat_columns.reduceByKey(lambda x,y: x+y).cache() # 2\n",
    "#     unique lhs + rhs rows are not needed to calculate possible combinations of 2 rows with equal lhs.\n",
    "    f_c_flat_columns = c_flat_columns.filter(lambda x: x[1] >= 2) # 3\n",
    "#     map identical lhs + rhs occurences to possible combs of 2\n",
    "    calc_combs = f_c_flat_columns.map(lambda x: map_value_to_combs_count(x)) # 4\n",
    "#     reduce amount of possible combinations of 2 equal lhs + rhs per lhs\n",
    "    reduce_combs_by_lhs = calc_combs.reduceByKey(lambda x,y: x+y) # 5\n",
    "    \n",
    "#     make use of already cached reduced rdd from step #2\n",
    "#     make sure to only reduce on lhs as we want total 2 comb count for lhs\n",
    "    row_c_for_lhs = c_flat_columns.map(lambda x: rem_rhs_value_from_key(x))\n",
    "    red_c_for_lhs = row_c_for_lhs.reduceByKey(lambda x,y: x+y)\n",
    "#     Filter out rows with unique lhs as they cannot match with another equal lhs row\n",
    "    filt_red_c_for_lhs = red_c_for_lhs.filter(lambda x: x[1] >= 2)\n",
    "#     calculate total number of 2 row combs per lhs\n",
    "    map_total_combs_lhs = filt_red_c_for_lhs.map(lambda x: map_value_to_combs_count(x))\n",
    "    \n",
    "#     now union the per (lhs, rhs) comb count rdd and per (lhs) comb count rdd\n",
    "    total_and_eq_combs = reduce_combs_by_lhs.union(reduce_combs_by_lhs)# 8\n",
    "#     calc percentage per lhs per FD\n",
    "#     this could be bottleneck as now every key only has 2 rows so low chance of being able to combine locally?\n",
    "    percentage_per_lhs = total_and_eq_combs.reduceByKey(lambda x,y: calc_percentage(x,y)) # 9\n",
    "    \n",
    "    mapped_percentages = percentage_per_lhs.map(lambda x: map_total_comb_to_zero_perc(x))\n",
    "#     reduce by value (percentage) per lhs. Keep separate count to calculate the mean over the percentages of one FD combination (mapValues).\n",
    "    means_percentages = mapped_percentages.mapValues(lambda value: (value, 1)) \\\n",
    "                                            .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "                                            .mapValues(lambda value: value[0]/value[1])\n",
    "    \n",
    "    filter_threshold_fds = means_percentages.filter(lambda x: x[1] >= perc_threshold)\n",
    "#     map to only Soft FD comb tuple\n",
    "    final_soft_fds = filter_threshold_fds.map(lambda x: x[0])\n",
    "#     form: [(\"lhs column\", \"lhs column\", \"rhs column\"), ....] depending on value of i in loop for amount of lhs columns\n",
    "    soft_fd_comb_list = final_soft_fds.collect()\n",
    "    \n",
    "# for testing locally\n",
    "#     if i == 1:\n",
    "#         print(len(soft_fd_comb_list))\n",
    "#         print(soft_fd_comb_list)\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69261572-7060-4447-99d8-661e4df2b25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdataman",
   "language": "python",
   "name": "bigdataman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
