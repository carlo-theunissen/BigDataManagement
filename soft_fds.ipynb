{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70eb68aa-71bb-4956-91d5-9602a0e9f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from configparser import ConfigParser\n",
    "\n",
    "config = ConfigParser()\n",
    "# create your own config.ini in root of project folder to store project configurations\n",
    "config.read('config.ini')\n",
    "\n",
    "pathfile = config.get('main', 'dirty_csv')  \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .appName(\"SparkFlight\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7270cd0-efa4-476b-89b2-a42953bf5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read preprocessed data (15mil rows)\n",
    "preproc_data = spark.read.csv('preprocessed_data.csv', inferSchema='true', header='true', mode='PERMISSIVE', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb48cb2-f356-4a9b-ad9c-b705f26286cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "from itertools import combinations, product\n",
    "\n",
    "def generate_deps(lhs_columns, rhs_columns, lhsSize: int, alreadyFound = []):\n",
    "    # generate left hand sides\n",
    "    lhss = combinations(lhs_columns, r=lhsSize)\n",
    "    # add right hand sides\n",
    "    deps = product(lhss, rhs_columns)\n",
    "    # convert to list format, where ['a', 'b', 'c'] corresponds to 'a', 'b' -> 'c'\n",
    "    deps = map(lambda dep: list(dep[0]) + [dep[1]], deps)\n",
    "\n",
    "    # remove tirivial dependencies (where RHS in LHS)\n",
    "    deps = filter(lambda dep: not dep[-1] in dep[:-1], deps)\n",
    "\n",
    "    # used to test if dependency is implied by some found dependency\n",
    "    includes_lhs = lambda found_dep, dep: all(attr in dep[:-1] for attr in found_dep[:-1])\n",
    "    implied = lambda dep: any(map(lambda found_dep: found_dep[-1] == dep[-1] and includes_lhs(found_dep, dep), alreadyFound))\n",
    "    # remove already found dependencies\n",
    "    deps = filter(lambda dep: not implied(dep), deps)\n",
    "\n",
    "    return list(deps)   \n",
    "\n",
    "def gen_contin_cells(x, bv_candidate_FDs, lhs_size):\n",
    "    combs = []\n",
    "    candidate_FDs = [tuple(candidate_FD) for candidate_FD in bv_candidate_FDs.value]\n",
    "#     for comb in candidate_FDs:\n",
    "    for i, fd in enumerate(candidate_FDs):\n",
    "# rows in form: (((lhs, rhs), (value(s) lhs, value rhs)), count)\n",
    "        combs.append(((i, tuple(x[column] for column in fd)),1))\n",
    "    return combs\n",
    "\n",
    "def calc_combinations(x):\n",
    "#     combinations of 2\n",
    "    return factorial(x)/(factorial(2)*factorial(x-2)) if x >= 2 else 0\n",
    "\n",
    "def map_value_to_combs_count(rdd_row):\n",
    "    lhs_plus_rhs = rdd_row[0][0]\n",
    "#     value of rhs is always last item. Strip it to only depend on value of lhs in key\n",
    "    values_lhs_tup = rdd_row[0][1][:-1]\n",
    "    reduced_count_rows = rdd_row[1]\n",
    "    return ((lhs_plus_rhs, values_lhs_tup), calc_combinations(reduced_count_rows))\n",
    "        \n",
    "def calc_percentage(x, y):\n",
    "#     normalize to percentage value in [0, 1]\n",
    "    result = y/x if x >= y else x/y\n",
    "#     if normalized percentage value = 1 then set to 1.5 instead so this value cannot be confused\n",
    "#     with a possible total 2 comb count of 1 for lhs\n",
    "#     as total combination count always has to be integer\n",
    "    result = 1.5 if result == 1.0 else result \n",
    "    return result\n",
    "\n",
    "def map_total_comb_to_zero_perc(rdd_row):\n",
    "    lhs_plus_rhs = rdd_row[0][0] \n",
    "    value = rdd_row[1]\n",
    "#     rdd_row could be row with total 2 comb count per lhs as value that did not get normalized to [0, 1] range as percentage\n",
    "#     as all rows for that lhs had unique rhs values so there was no row with possible combinations of 2 equal lhs + rhs per lhs\n",
    "#     to reduce with. Rows with unique lhs + rhs were filtered in step #3. \n",
    "#     lhs that has no possible combinations of 2 equal lhs + rhs per lhs will be set to 0%\n",
    "#     otherwise rdd_row has normalized percentage as value but in the case of percentage being 100% value = 1.5 instead of 1.\n",
    "    if value >= 1 and value != 1.5:\n",
    "        value = 0\n",
    "    elif value == 1.5: # set 100%'s back to 1\n",
    "        value = 1\n",
    "    return (lhs_plus_rhs, value)\n",
    "    \n",
    "def rem_rhs_value_from_key(rdd_row):\n",
    "    lhs_plus_rhs = rdd_row[0][0]\n",
    "#     value of rhs is always last item. Strip it to only depend on value of lhs in key\n",
    "    values_lhs_tup = rdd_row[0][1][:-1]\n",
    "    value = rdd_row[1]\n",
    "    return ((lhs_plus_rhs, values_lhs_tup), value)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd315bf5-c73d-445e-84f4-b34e7d7a590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  get a sample. This is just for testing locally. Should be whole preprocessed dataset.\n",
    "# len_sample = 16_000\n",
    "# len_preproc_data = 15_000_000\n",
    "# cont_sample_data = preproc_data.sample(True, fraction=len_sample/len_preproc_data).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f8f4e-f6bc-4852-b0f3-7e307aab5ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running full dataset over 342 possible Soft Dependencies with threshold: 0.7 and lhs: 1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "max_lhs = 3\n",
    "col_names = preproc_data.columns\n",
    "found_soft_dep = []\n",
    "perc_threshold = 0.7\n",
    "\n",
    "for i in range(1, max_lhs+1):\n",
    "    tic = time.perf_counter()\n",
    "    candidate_FDs = generate_deps(col_names, col_names, i, found_soft_dep)\n",
    "#     broadcast the candidate fds only once to all nodes: http://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables\n",
    "    broadcast_candidate_FDs = spark.sparkContext.broadcast(candidate_FDs)\n",
    "    \n",
    "    print(f\"Running full dataset over {len(candidate_FDs)} possible Soft Dependencies with threshold: {perc_threshold} and lhs: {i}\")\n",
    "# #    sample only for local use to test\n",
    "#     flat_columns = cont_sample_data.rdd.flatMap(lambda x: gen_contin_cells(x, broadcast_candidate_FDs, lhs_size=i)) # 1\n",
    "\n",
    "#     create all column combs per row\n",
    "    flat_columns = preproc_data.rdd.flatMap(lambda x: gen_contin_cells(x, broadcast_candidate_FDs, lhs_size=i)) # 1\n",
    "#     cache or not?? only used one time extra this rdd later? check if this actually wins time\n",
    "    c_flat_columns = flat_columns.reduceByKey(lambda x,y: x+y).cache() # 2\n",
    "#     unique lhs + rhs rows are not needed to calculate possible combinations of 2 rows with equal lhs.\n",
    "    f_c_flat_columns = c_flat_columns.filter(lambda x: x[1] >= 2) # 3\n",
    "#     map identical lhs + rhs occurences to possible combs of 2\n",
    "    calc_combs = f_c_flat_columns.map(lambda x: map_value_to_combs_count(x)) # 4\n",
    "#     reduce amount of possible combinations of 2 equal lhs + rhs per lhs\n",
    "    reduce_combs_by_lhs = calc_combs.reduceByKey(lambda x,y: x+y) # 5\n",
    "    \n",
    "#     make use of already cached reduced rdd from step #2\n",
    "#     make sure to only reduce on lhs as we want total 2 comb count for lhs\n",
    "    row_c_for_lhs = c_flat_columns.map(lambda x: rem_rhs_value_from_key(x))\n",
    "    red_c_for_lhs = row_c_for_lhs.reduceByKey(lambda x,y: x+y)\n",
    "#     Filter out rows with unique lhs as they cannot match with another equal lhs row\n",
    "    filt_red_c_for_lhs = red_c_for_lhs.filter(lambda x: x[1] >= 2)\n",
    "#     calculate total number of 2 row combs per lhs\n",
    "    map_total_combs_lhs = filt_red_c_for_lhs.mapValues(lambda value: calc_combinations(value))\n",
    "    \n",
    "#     now union the per (lhs, rhs) comb count rdd and per (lhs) comb count rdd\n",
    "    total_and_eq_combs = map_total_combs_lhs.union(reduce_combs_by_lhs)# 8\n",
    "#     calc percentage per lhs per FD\n",
    "#     this could be bottleneck as now every key only has 2 rows so low chance of being able to combine locally?\n",
    "    percentage_per_lhs = total_and_eq_combs.reduceByKey(lambda x,y: calc_percentage(x,y)) # 9\n",
    "    \n",
    "    mapped_percentages = percentage_per_lhs.map(lambda x: map_total_comb_to_zero_perc(x))\n",
    "#     reduce by value (percentage) per lhs. Keep separate count to calculate the mean over the percentages of one FD combination (mapValues).\n",
    "    means_percentages = mapped_percentages.mapValues(lambda value: (value, 1)) \\\n",
    "                                            .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "                                            .mapValues(lambda value: value[0]/value[1])\n",
    "    \n",
    "    filter_threshold_fds = means_percentages.filter(lambda x: x[1] >= perc_threshold)\n",
    "#     take only fd index and map index to full fd\n",
    "    take_only_fd_indic = filter_threshold_fds.map(lambda x: broadcast_candidate_FDs.value[x[0]])\n",
    "    soft_fd_comb_list = take_only_fd_indic.collect()\n",
    "#     form: [[\"lhs column\", \"lhs column\", \"rhs column\"], ....] depending on value of i in loop for amount of lhs columns\n",
    "    found_soft_dep += soft_fd_comb_list\n",
    "    \n",
    "    toc = time.perf_counter()\n",
    "    print(f\"discovering soft dependencies took {toc - tic:0.4f} seconds\")\n",
    "    print(f'amount of dependencies found: {len(found_soft_dep)}')\n",
    "    print('found soft dependencies:')\n",
    "    print(found_soft_dep)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4908ca24-6890-4a9d-83eb-b1ccf1f8413d",
   "metadata": {},
   "source": [
    "# These cells are just for testing locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383cc723-c99e-4441-af77-12789cd76644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "col_names = preproc_data.columns\n",
    "comb_dict = gen_col_combs(col_names,3)\n",
    "len(comb_dict['lhs_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ab51a-ceb8-4567-b58f-d0ac663fdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_combs = [('TailNum', 'SecurityDelay'), ('CRSArrTimestamp', 'NASAndWeatherDelay'), ('Distance', 'SecurityDelay'), ('UniqueCarrier', 'LateAircraftAndCarrierDelay'), ('AirTime', 'SecurityDelay'), ('Origin', 'NASAndWeatherDelay'), ('Origin', 'SecurityDelay'), ('ArrTimestamp', 'LateAircraftAndCarrierDelay'), ('ActualElapsedTime', 'NASAndWeatherDelay'), ('Dest', 'SecurityDelay'), ('CRSDepTimeStamp', 'LateAircraftAndCarrierDelay'), ('LateAircraftAndCarrierDelay', 'SecurityDelay'), ('ArrTimestamp', 'SecurityDelay'), ('Distance', 'NASAndWeatherDelay'), ('DepTimestamp', 'NASAndWeatherDelay'), ('ActualElapsedTime', 'SecurityDelay'), ('Dest', 'NASAndWeatherDelay'), ('CRSElapsedTime', 'SecurityDelay'), ('TaxiOut', 'LateAircraftAndCarrierDelay'), ('ArrTimestamp', 'NASAndWeatherDelay'), ('CRSArrTimestamp', 'SecurityDelay'), ('UniqueCarrier', 'SecurityDelay'), ('Distance', 'LateAircraftAndCarrierDelay'), ('TaxiIn', 'LateAircraftAndCarrierDelay'), ('TaxiOut', 'SecurityDelay'), ('Dest', 'LateAircraftAndCarrierDelay'), ('CRSArrTimestamp', 'LateAircraftAndCarrierDelay'), ('TailNum', 'NASAndWeatherDelay'), ('ArrDelay', 'SecurityDelay'), ('NASAndWeatherDelay', 'SecurityDelay'), ('TaxiIn', 'SecurityDelay'), ('DepTimestamp', 'LateAircraftAndCarrierDelay'), ('UniqueCarrier', 'NASAndWeatherDelay'), ('AirTime', 'NASAndWeatherDelay'), ('CRSDepTimeStamp', 'NASAndWeatherDelay'), ('Origin', 'LateAircraftAndCarrierDelay'), ('TailNum', 'UniqueCarrier'), ('CRSElapsedTime', 'NASAndWeatherDelay'), ('ActualElapsedTime', 'LateAircraftAndCarrierDelay'), ('CRSElapsedTime', 'LateAircraftAndCarrierDelay'), ('DepTimestamp', 'SecurityDelay'), ('DepDelay', 'SecurityDelay'), ('TailNum', 'LateAircraftAndCarrierDelay'), ('AirTime', 'LateAircraftAndCarrierDelay'), ('CRSDepTimeStamp', 'SecurityDelay')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eeaf83-ba4f-4e93-a4c2-cdb2ce01cfa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# comb_list_lhs1 = generate_deps(col_names, col_names, 1, [])\n",
    "# print(len(comb_list_lhs1))\n",
    "# comb_list_lhs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f6344-04ae-40f3-8451-294d1972be4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comb_list_lhs2 = generate_deps(col_names, col_names, 2, [])\n",
    "print(len(comb_list_lhs2))\n",
    "comb_list_lhs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f46856-e5e1-4bb0-9506-232ae9c3dc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comb_list_lhs2_min_found_with_lhs1 = generate_deps(col_names, col_names, 2, found_combs)\n",
    "# comb_list_lhs2_min_found_with_lhs1 = generate_deps(col_names, col_names, 2, [])\n",
    "# comb_list_lhs2_min_found_with_lhs1 = generate_deps(col_names, col_names, 1, found_combs)\n",
    "print(len(comb_list_lhs2_min_found_with_lhs1))\n",
    "comb_list_lhs2_min_found_with_lhs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff1156-942a-472b-b1d7-930fe6d3eaea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "found_combs_transf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdataman",
   "language": "python",
   "name": "bigdataman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
